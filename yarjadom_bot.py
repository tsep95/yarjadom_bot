import os
import logging
from telegram import Update, Bot
from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext, ConversationHandler
import openai

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÑÑ‚Ð°Ð¿Ð¾Ð² Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°
GREETING, ANALYSIS, DEEP_ANALYSIS, SOLUTION, SUBSCRIPTION = range(5)

# ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")
TELEGRAM_TOKEN = os.getenv("TELEGRAM_API_TOKEN")

# Ð¥Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ (Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð´Ð°ÐºÑˆÐµÐ½Ð° Ð»ÑƒÑ‡ÑˆÐµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð‘Ð”)
user_states = {}

logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO
)

async def start(update: Update, context: CallbackContext) -> int:
    welcome_message = (
        "ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ Ñ€ÑÐ´Ð¾Ð¼. ðŸ¤—\n"
        "Ð¢Ñ‘Ð¿Ð»Ñ‹Ð¹ Ð¿ÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº, Ñ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¼ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ð¾Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ. ðŸ§¸\n\n"
        "Ð•ÑÐ»Ð¸ Ñ‚ÐµÐ±Ðµ Ñ‚ÑÐ¶ÐµÐ»Ð¾, Ñ‚Ñ€ÐµÐ²Ð¾Ð¶Ð½Ð¾, Ð¿ÑƒÑÑ‚Ð¾ Ð¸Ð»Ð¸ Ð½Ðµ Ñ ÐºÐµÐ¼ Ð¿Ð¾Ð´ÐµÐ»Ð¸Ñ‚ÑŒÑÑ â€” Ð¿Ð¸ÑˆÐ¸. âœï¸\n"
        "Ð¯ Ð½Ðµ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÑŽ, Ð½Ðµ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÑƒÑŽ, Ð½Ðµ Ð·Ð°ÑÑ‚Ð°Ð²Ð»ÑÑŽ. Ð¯ Ñ€ÑÐ´Ð¾Ð¼, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð°Ñ‚ÑŒ. ðŸ’›\n\n"
        "ðŸ’¬ ÐœÐ¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° â€” Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ‚ÐµÐ±Ðµ Ð¿Ð¾Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐµÐ±Ñ Ð»ÑƒÑ‡ÑˆÐµ Ð¿Ñ€ÑÐ¼Ð¾ ÑÐµÐ¹Ñ‡Ð°Ñ.\n"
        "ÐœÑ‹ Ð¼Ð¾Ð¶ÐµÐ¼ Ð¼ÑÐ³ÐºÐ¾ Ñ€Ð°Ð·Ð¾Ð±Ñ€Ð°Ñ‚ÑŒÑÑ, Ñ‡Ñ‚Ð¾ Ñ‚ÐµÐ±Ñ Ð±ÐµÑÐ¿Ð¾ÐºÐ¾Ð¸Ñ‚, Ð¸ Ð½Ð°Ð¹Ñ‚Ð¸, Ñ‡Ñ‚Ð¾ Ñ ÑÑ‚Ð¸Ð¼ Ð¼Ð¾Ð¶Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ. ðŸ•Šï¸ðŸ§ \n\n"
        "ðŸ”’ Ð‘Ð¾Ñ‚ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð½Ñ‹Ð¹ â€” Ñ‚Ñ‹ Ð¼Ð¾Ð¶ÐµÑˆÑŒ Ð±Ñ‹Ñ‚ÑŒ ÑÐ¾Ð±Ð¾Ð¹.\n\n"
        "Ð¥Ð¾Ñ‡ÐµÑˆÑŒ â€” Ð½Ð°Ñ‡Ð½Ñ‘Ð¼ Ñ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð³Ð¾: Ñ€Ð°ÑÑÐºÐ°Ð¶Ð¸, ÐºÐ°Ðº Ñ‚Ñ‹ ÑÐµÐ¹Ñ‡Ð°Ñ? ðŸŒ¤ï¸ðŸ’¬"
    )
    
    await update.message.reply_text(welcome_message)
    user_states[update.effective_chat.id] = {
        "stage": GREETING,
        "history": [{"role": "system", "content": "Ð¢Ñ‹ Ð¾Ð¿Ñ‹Ñ‚Ð½Ñ‹Ð¹ Ð¿ÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³..."}]
    }
    return GREETING

def generate_gpt_response(prompt: str, chat_id: int) -> str:
    try:
        user_states[chat_id]["history"].append({"role": "user", "content": prompt})
        
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=user_states[chat_id]["history"],
            temperature=0.7,
            max_tokens=500
        )
        
        assistant_reply = response.choices[0].message['content']
        user_states[chat_id]["history"].append({"role": "assistant", "content": assistant_reply})
        
        return assistant_reply
    
    except Exception as e:
        logging.error(f"OpenAI Error: {e}")
        return "ÐšÐ°Ð¶ÐµÑ‚ÑÑ, Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð½Ðµ Ð¼Ð¾Ð³Ñƒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ. ðŸ«£ Ð”Ð°Ð²Ð°Ð¹ Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·?"

async def handle_message(update: Update, context: CallbackContext) -> int:
    chat_id = update.effective_chat.id
    user_input = update.message.text
    
    if chat_id not in user_states:
        return await start(update, context)
    
    current_stage = user_states[chat_id]["stage"]
    
    # Ð›Ð¾Ð³Ð¸ÐºÐ° Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ð° Ð¼ÐµÐ¶Ð´Ñƒ ÑÑ‚Ð°Ð¿Ð°Ð¼Ð¸
    if current_stage == GREETING:
        response = generate_gpt_response(f"ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ð»: {user_input}. ÐÐ°Ñ‡Ð½Ð¸ ÑÐµÑÑÐ¸ÑŽ Ñ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° 'Ð´Ð°/Ð½ÐµÑ‚' Ð¿Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ð¸ÐºÐµ ÐšÐŸÐ¢", chat_id)
        user_states[chat_id]["stage"] = ANALYSIS
    
    elif current_stage == ANALYSIS:
        response = generate_gpt_response(f"ÐžÑ‚Ð²ÐµÑ‚: {user_input}. Ð—Ð°Ð´Ð°Ð¹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑƒÑ‚Ð¾Ñ‡Ð½ÑÑŽÑ‰Ð¸Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ 'Ð´Ð°/Ð½ÐµÑ‚'", chat_id)
        user_states[chat_id]["stage"] = DEEP_ANALYSIS
    
    elif current_stage == DEEP_ANALYSIS:
        response = generate_gpt_response(f"ÐžÑ‚Ð²ÐµÑ‚: {user_input}. ÐŸÐµÑ€ÐµÐ¹Ð´Ð¸ Ðº Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð¼Ñƒ Ð°Ð½Ð°Ð»Ð¸Ð·Ñƒ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹", chat_id)
        user_states[chat_id]["stage"] = SOLUTION
    
    elif current_stage == SOLUTION:
        response = generate_gpt_response(f"ÐžÑ‚Ð²ÐµÑ‚: {user_input}. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð¸ Ð¼ÑÐ³ÐºÐ¾ ÑƒÐ¿Ð¾Ð¼ÑÐ½Ð¸ Ð¾ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐµ", chat_id)
        user_states[chat_id]["stage"] = SUBSCRIPTION
    
    else:
        response = generate_gpt_response(user_input, chat_id)
    
    await update.message.reply_text(response)
    return user_states[chat_id]["stage"]

async def cancel(update: Update, context: CallbackContext) -> int:
    await update.message.reply_text("Ð’ÑÐµÐ³Ð´Ð° Ð±ÑƒÐ´Ñƒ Ñ€Ð°Ð´ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ ÑÐ½Ð¾Ð²Ð°! ðŸ’–")
    return ConversationHandler.END

def main() -> None:
    application = Application.builder().token(TELEGRAM_TOKEN).build()

    conv_handler = ConversationHandler(
        entry_points=[CommandHandler("start", start)],
        states={
            GREETING: [MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message)],
            ANALYSIS: [MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message)],
            DEEP_ANALYSIS: [MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message)],
            SOLUTION: [MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message)],
            SUBSCRIPTION: [MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message)]
        },
        fallbacks=[CommandHandler("cancel", cancel)],
    )

    application.add_handler(conv_handler)
    application.run_polling()

if __name__ == "__main__":
    main()
